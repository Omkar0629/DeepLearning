{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1674ab-9a07-4e33-b9af-846c92ffcb08",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "    1. Name: Omkar Pawar\n",
    "    2. Batch: R-9\n",
    "    3. Roll No.: 43160\n",
    "    \n",
    "### Problem Statement :\n",
    "    Implement the Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42893e0f-9d09-4570-a193-ad8be00fccb8",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eee3578-709e-4702-b601-b45de3b8e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utilsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
      "     ---------------------------------------- 0.0/62.0 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/62.0 kB 682.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.0/62.0 kB 834.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.0 in e:\\sem_7\\lp-4\\virtual_env\\lib\\site-packages (from np_utils) (1.26.0)\n",
      "Building wheels for collected packages: np_utils\n",
      "  Building wheel for np_utils (setup.py): started\n",
      "  Building wheel for np_utils (setup.py): finished with status 'done'\n",
      "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56449 sha256=3487c222825b9a030b9fa9068d9aabee85b261addbc909c72b561f2aacd06641\n",
      "  Stored in directory: c:\\users\\pawar\\appdata\\local\\pip\\cache\\wheels\\19\\0d\\33\\eaa4dcda5799bcbb51733c0744970d10edb4b9add4f41beb43\n",
      "Successfully built np_utils\n",
      "Installing collected packages: np_utils\n",
      "Successfully installed np_utils-0.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e309c5-9985-439e-b3f3-431477ecef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.src.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073073a-9382-4942-a6fb-18f711c5f930",
   "metadata": {},
   "source": [
    "#### Random sentence as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6386614-2a2b-410a-879a-e82bdccc7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a \n",
    "broader family of machine learning methods based on artificial neural networks \n",
    "with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, \n",
    "deep reinforcement learning, recurrent neural networks, convolutional neural networks and \n",
    "Transformers have been applied to fields including computer vision, speech recognition, \n",
    "natural language processing, machine translation, bioinformatics, drug design, \n",
    "medical image analysis, climate science, material inspection and board game programs, \n",
    "where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21614c-1cd3-40cb-9761-b94e9d50c0da",
   "metadata": {},
   "source": [
    "### a. Data preparation\n",
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866f78eb-4ea3-4e1c-b950-651d46faf941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196534f-23e6-49af-a87e-5bdb70b279dc",
   "metadata": {},
   "source": [
    "#### b. Generate training data\n",
    "#### Generating (context word, target/label word) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b079c61-6b19-4e0c-9bb4-4e4e63277289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i]\n",
    "                                 for i in range(start, end)\n",
    "                                 if 0 <= i < sentence_length\n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c856d-405f-4349-a8ed-2be9b0cd2635",
   "metadata": {},
   "source": [
    "### c. Train Model\n",
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bd802e-04f7-491d-963d-f1de265ea879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15075 (58.89 KB)\n",
      "Trainable params: 15075 (58.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5a4aac-4adc-4dfb-8b67-a7c0f85625dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 433.3096146583557\n",
      "\n",
      "Epoch: 2 \tLoss: 428.84291982650757\n",
      "\n",
      "Epoch: 3 \tLoss: 425.5224049091339\n",
      "\n",
      "Epoch: 4 \tLoss: 422.4686472415924\n",
      "\n",
      "Epoch: 5 \tLoss: 420.1378016471863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f51138f-538f-4940-b22b-80d3100c31b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.013229</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>-0.020026</td>\n",
       "      <td>-0.020566</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.006816</td>\n",
       "      <td>0.035224</td>\n",
       "      <td>-0.057148</td>\n",
       "      <td>0.034406</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059610</td>\n",
       "      <td>0.060229</td>\n",
       "      <td>0.050503</td>\n",
       "      <td>-0.043293</td>\n",
       "      <td>0.030989</td>\n",
       "      <td>0.049634</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>-0.023802</td>\n",
       "      <td>-0.050265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>-0.051126</td>\n",
       "      <td>-0.012177</td>\n",
       "      <td>0.013474</td>\n",
       "      <td>-0.030006</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.039106</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>-0.014620</td>\n",
       "      <td>0.041402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.024922</td>\n",
       "      <td>-0.004864</td>\n",
       "      <td>-0.058565</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>-0.003913</td>\n",
       "      <td>-0.062047</td>\n",
       "      <td>0.038528</td>\n",
       "      <td>-0.026459</td>\n",
       "      <td>-0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.048150</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>-0.032853</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>-0.011879</td>\n",
       "      <td>-0.026698</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>-0.041267</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-0.001523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041538</td>\n",
       "      <td>-0.043489</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.023040</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>-0.048479</td>\n",
       "      <td>-0.042032</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>0.023323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.038757</td>\n",
       "      <td>-0.049871</td>\n",
       "      <td>-0.044562</td>\n",
       "      <td>0.026347</td>\n",
       "      <td>-0.022240</td>\n",
       "      <td>-0.048286</td>\n",
       "      <td>0.031216</td>\n",
       "      <td>-0.039120</td>\n",
       "      <td>-0.025615</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>-0.014461</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>0.044381</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.028213</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.033509</td>\n",
       "      <td>0.040212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.030740</td>\n",
       "      <td>0.049739</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>-0.025917</td>\n",
       "      <td>-0.026471</td>\n",
       "      <td>0.005818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-0.005880</td>\n",
       "      <td>0.027980</td>\n",
       "      <td>-0.029601</td>\n",
       "      <td>0.019891</td>\n",
       "      <td>0.037088</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>0.035577</td>\n",
       "      <td>0.040180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "deep     -0.013229  0.016762 -0.020026 -0.020566  0.031135  0.006816   \n",
       "networks -0.051126 -0.012177  0.013474 -0.030006 -0.000113 -0.039106   \n",
       "neural    0.048150  0.013395 -0.032853  0.020569 -0.011879 -0.026698   \n",
       "and      -0.038757 -0.049871 -0.044562  0.026347 -0.022240 -0.048286   \n",
       "as        0.016282  0.036789  0.030740  0.049739  0.023463  0.000273   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "deep      0.035224 -0.057148  0.034406  0.007027  ... -0.059610  0.060229   \n",
       "networks  0.001721  0.033965 -0.014620  0.041402  ...  0.029056  0.024922   \n",
       "neural    0.034642 -0.041267 -0.000735 -0.001523  ...  0.041538 -0.043489   \n",
       "and       0.031216 -0.039120 -0.025615  0.007222  ...  0.004701 -0.014461   \n",
       "as        0.040475 -0.025917 -0.026471  0.005818  ... -0.004882 -0.005880   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "deep      0.050503 -0.043293  0.030989  0.049634  0.041702  0.015633   \n",
       "networks -0.004864 -0.058565  0.002117 -0.003913 -0.062047  0.038528   \n",
       "neural   -0.005860  0.023800  0.023040  0.032298 -0.048479 -0.042032   \n",
       "and       0.044197  0.044381  0.012224  0.043667  0.028213  0.005074   \n",
       "as        0.027980 -0.029601  0.019891  0.037088 -0.001953  0.028963   \n",
       "\n",
       "                98        99  \n",
       "deep     -0.023802 -0.050265  \n",
       "networks -0.026459 -0.002042  \n",
       "neural    0.030212  0.023323  \n",
       "and       0.033509  0.040212  \n",
       "as        0.035577  0.040180  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b479a-032b-46bc-8139-f3e94acd908c",
   "metadata": {},
   "source": [
    "### d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd3f416-2268-4612-9f86-7f0e1f7c889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['transformers', 'they', 'game', 'of', 'such']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "                   for search_term in ['deep']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090e381-c0ad-499f-bd92-df53ab903345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_kernel",
   "language": "python",
   "name": "virtual_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
